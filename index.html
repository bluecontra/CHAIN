
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn">
  <meta name="keywords" content="Churn, Deep RL, Instability, Function Approximation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-B37BFDTB3H"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-B37BFDTB3H');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro|Source+Code+Pro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <!-- <link rel="stylesheet" href="./static/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script> -->
  <!-- <script src="./static/js/index.js"></script> -->
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://aditya.bhatts.org/sensorless-in-hand-manipulation">
            Surprisingly Robust In-Hand Manipulation
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            CrossQ:
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div> 

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="is-size-5 publication-authors">
            <span class="author-block spotlight"><span style="font-weight: bold; color: orange !important;">NeurIPS 2024</span></span>
            <!-- <span class="author-block spotlight">NeurIPS 2024 • <span style="font-weight: bold; color: orange !important;">top 5%</span> • spotlight </span> -->
          </div>
          <h1 class="title is-size-3 publication-title"><span style="font-weight: bold; color: rgb(250, 250, 250) !important;">Improving Deep Reinforcement Learning<br/> by Reducing the Chain Effect of Value and Policy Churn</span></h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://bluecontra.github.io"><span style="font-weight: bold; color: rgb(93, 240, 248) !important;">Hongyao Tang</span></a><sup>1,2</sup></span>
            <span class="author-block">
              <a href="https://neo-x.github.io/"><span style="font-weight: bold; color: rgb(93, 240, 248) !important;">Glen Berseth</span></a><sup>1,2</sup></span>
          </div>

          <div class="is-size-5 publication-authors publication-affiliation">
            <!-- <span class="author-block"><sup>*</sup>Equal contribution</span> -->
            <span class="author-block"><sup>1</sup><span style="color: rgb(250, 250, 250) !important;">Mila - Quebec AI Institute</span></span>
            <span class="author-block"><sup>2</sup><span style="color: rgb(250, 250, 250) !important;">Université de Montréal</span></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/pdf?id=cQoAgPBARc"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="talk_recording_goes_here"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/bluecontra/CHAIN"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>


          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
            <p>
            Deep neural networks introduce challenges due to the non-stationary nature of RL training.
            One source of the challenges in RL is that output predictions can <i>churn</i>, leading to <em>uncontrolled changes</em> after each batch update for <em>states not included in the batch</em>.
            Although such a churn phenomenon exists in each step of network training, how churn occurs and impacts RL remains under-explored.
            </p>
            <p>
            In this work, we aim to answer <em>how churn occurs and influence the learning process of DRL agents</em>, and <em>how to deal with churn for better learning performance</em>.
            The contributions are summarized below:
            </p>
            <ol>
              <li>We formally define and study the policy and value churn under GPI framework and present <em>the chain effect of churn</em>, which induces compounding churns and biases throughout learning.</li>
              <li>We show how churn results in three learning issues in typical DRL settings, including Greedy-action Deviation, Trust Region Violation, and Dual Bias in Policy Value.</li>
              <li>We propose a general method to reduce the chain effect, called <em>CH</em>urn <em>A</em>pproximated Reduct<em>I</em>o<em>N</em> (<chain>CHAIN</chain>), which can be easily plugged into most existing DRL algorithms.
              In our experiments, we show that <chain>CHAIN</chain> effectively <em>alleviates the learning issues</em>, <em>improves efficiency and final scores</em> in various settings, and <em>facilitates the scaling of PPO</em>.</li>
            </ol>
            </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
  <!-- <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <img src="./static/images/efficiency_sample_compute.png"/>
      </div>
    </div>
  </div> -->

</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TL;DR</h2>
        <div class="content has-text-justified">
            <h3 class="title is-4">What is Churn?</h3>
            <p>
            In general, churn is defined as <em>the difference between the prior-update network prediction and the post-update network prediction</em>
            for the data <em>NOT</em> in the batch used to make the update:
            </p>
            <div class="container is-max-desktop">
              <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                  <img class='paper-img' src="./static/images/churn_def.png" width="80%"/>
                </div>
              </div>
            </div>
            <p>
            In the context of deep RL, we care about the <em>value churn</em> and the <em>policy churn</em>:
            </p>
            <div class="container is-max-desktop">
              <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                  <img class='paper-img' src="./static/images/value_and_policy_churn_def.png" width="80%"/>
                </div>
              </div>
            </div>
            <!-- <ul>
              <li>
                UTD=1 methods — like SAC and TD3 — are fast, but <em>not sample-efficient enough</em>
              </li>
              <li>
                UTD=20 methods — like REDQ and DroQ — are sample-efficient, but <em>not fast enough</em>
              </li>
              <li>
                High-UTD training <em>requires Q-function bias reduction</em>, making algorithms more complex
              </li>
            </ul> -->
            
            <h3 class="title is-4">Where is Churn in Deep RL?</h3>
            <p>Let's explicitly model the churn in a general form with the <em>Generalized Policy Iteration (GPI)</em> framework. 
            Compared to the standard GPI where churn is neglected, now we have the churn in the iterative training process of both the value function and policy function.
            </p>
            <p>As illustrated, we consider <em>both the exact updates</em> for the data in training batches and <em>the implicit changes caused by churn</em> for out-of-batch data.</p>
            <div class="container is-max-desktop">
              <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                  <img class='paper-img' src="./static/images/gpi_comparison.png"/>
                  <span style="color: grey !important;">Standard GPI [Sutton and Barto, 2018] (<i>left</i>) v.s. GPI under churn (<i>right</i>)</span>
                </div>
              </div>
            </div>

            <h3 class="title is-4">How the policy and value churn occur and influence the learning?</h3>
            <span style="font-weight: bold; color: black !important;">The Chain Effect of Churn</span>
            <ol>
              <li>The <em>network parameter updates causes</em> the value and policy <em>churn</em>.</li>
              <li>The policy churn and value <em>churn further lead to the deviations</em> in the action gradient and policy value.</li>
              <li>Then, the churns and the deviations <em>bias the following parameter updates</em>.</li>
              <!-- <li>
                Both batches in TD have <em>different statistics</em>: <em>A</em> comes from the replay, <em>A'</em> comes from the policy
              </li>
              <li>
                BatchNorm's <em>mismatched running statistics</em> degrade Q predictions and harm training
              </li> -->
            </ol>
            <p>The chain effect forms a cycle and lasts throughout learning, where the churn and biases can <em>accumulate and amplify each other</em>.</p>
            
            <span style="font-weight: bold; color: black !important;">The Issues Caused by Churn in Popular Deep RL Agents</span>
            <ul>
              <li><em>Greedy action deviation</em> in value-based methods</li>
              <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                  <div class="column is-four-fifths">
                    <img class='paper-img' src="./static/images/greedy_action_deviation.png" width="60%"/>
                  </div>
                </div>
              </div>
              <li><em>Trust region violation</em> in policy gradient methods</li>
              <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                  <div class="column is-four-fifths">
                    <img class='paper-img' src="./static/images/trust_region_violation.png" width="80%"/>
                  </div>
                </div>
              </div>
              <li><em>Dual bias of policy value</em> in AC methods</li>
              <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                  <div class="column is-four-fifths">
                    <img class='paper-img' src="./static/images/dual_bias_of_policy_value.png" width="45%"/>
                  </div>
                </div>
              </div>
            </ul>

            <h3 class="title is-4">Regularizing Churn for Better Deep RL</h3>
            We propose a simple regularization method, called <em>Churn Approximated Reduction</em> (<chain>CHAIN</chain>), to reduce churn by holding a conservative attitude on the churn. 
            <ol>
              <li>Sample a separate batch alongside the conventional training batch, called <em>reference batch</em>.</li>
              <li>Compute the churn amount with the reference batch as <em>the churn regularization term</em>.</li>
              <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                  <div class="column is-four-fifths">
                    <img class='paper-img' src="./static/images/regularization_term.png" width="80%"/>
                  </div>
                </div>
              </div>
              <li>(Optional) Adjust the regularization coefficient according to the relative scale between the regularization term and the conventional deep RL objectives.</li>
              <li>Minimize the churn regularization terms <em>together with</em> the conventional RL objectives.</li>
              <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                  <div class="column is-four-fifths">
                    <img class='paper-img' src="./static/images/chain_loss_func.png" width="65%"/>
                  </div>
                </div>
              </div>
            </ol>
            
            <!-- <h3 class="title is-4"><crossq>CrossQ</crossq></h3>
            To turn SAC into <crossq>CrossQ</crossq>, we make <em>3 key changes</em>:
            <ol>
              <li><em>Delete target nets</em>, simplifying the algorithm</li>
              <li><em>Use batch normalization</em>, boosting sample-efficiency by an order of magnitude</li>
              <li><em>Widen the critic</em>, further improving performance</li>
            </ol>
            <p>
            These changes take only a few lines of code.
            </p>
            <img class='paper-img' src="./static/images/efficiency_sample.png"/> -->
            
            
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
            
            <h3 class="title is-4"><chain>CHAIN</chain> Reduces the Deviation of Greedy Action</h3>
            <p>For DoubleDQN and MinAtar environments, churn induces a decrease in the value of greedy action, thus degrading to inferior actions.
              While <chain>CHAIN</chain> significantly prevents it and leads to higher efficiency and better final scores.</p>
              <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                  <div class="column is-four-fifths">
                    <img class='paper-img' src="./static/images/chain_ddqn_fig.png"/>
                    <span style="color: grey !important;">DoubleDQN (DDQN) v.s., <chain>CHAIN</chain>-DDQN in MinAtar tasks.</span>
                  </div>
                </div>
              </div>
              <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                  <div class="column is-four-fifths">
                    <img class='paper-img' src="./static/images/chain_ddqn_value_churn.png"/>
                    <span style="color: grey !important;">DDQN v.s., <chain>CHAIN</chain>-DDQN in terms of:
                      (from left to right) greedy action deviation (%), value change of all actions, value change of greedy action.
                     </span>
                  </div>
                </div>
              </div>

            <h3 class="title is-4"><chain>CHAIN</chain> Reduces Policy Churn and Clip Range Violation</h3>
            <p>For PPO and continuous control tasks in MuJoCo and DMC, <chain>CHAIN</chain> reduces the policy churn effectively and suppresses the violation of clip range, leading to improved learning performance in terms of efficiency and higher scores.</p>
            <div class="container is-max-desktop">
              <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                  <img class='paper-img' src="./static/images/chain_ppo_fig.png"/>
                  <span style="color: grey !important;">PPO v.s., <chain>CHAIN</chain>-PPO in MuJoCo and DMC tasks.</span>
                </div>
              </div>
            </div>
            <div class="container is-max-desktop">
              <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                  <img class='paper-img' src="./static/images/chain_policy_reduce_fig.png"/>
                  <span style="color: grey !important;"><chain>CHAIN</chain>-PPO with different static regularization coefficients.
                    From left to right: episode return, policy churn, policy loss, and the regularization term.
                  (The policy churn diminishes as the decay of learning rate.)</span>
                </div>
              </div>
            </div>

            <h3 class="title is-4"><chain>CHAIN</chain> Facilitates the Scaling of PPO</h3>
            <p><chain>CHAIN</chain> alleviates the deterioration of PPO when using larger networks, no matter scaling an MLP-based PPO agent by widening or deepening.
              We think it is appealing as it reveals some promise to address the scalability issue by controlling churn or generalization.</p>
              <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                  <div class="column is-four-fifths">
                    <img class='paper-img' src="./static/images/chain_scaling_fig.png"/>
                    <span style="color: grey !important;">Scaling PPO with <chain>CHAIN</chain> across different scale and learning rate configurations.</span>
                  </div>
                </div>
              </div>
              <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                  <div class="column is-four-fifths">
                    <img class='paper-img' src="./static/images/chain_scaling_table.png"/>
                  </div>
                </div>
              </div>
            <!-- <h3 class="title is-4"><crossq>CrossQ</crossq></h3>
            To turn SAC into <crossq>CrossQ</crossq>, we make <em>3 key changes</em>:
            <ol>
              <li><em>Delete target nets</em>, simplifying the algorithm</li>
              <li><em>Use batch normalization</em>, boosting sample-efficiency by an order of magnitude</li>
              <li><em>Widen the critic</em>, further improving performance</li>
            </ol>
            <p>
            These changes take only a few lines of code.
            </p>
            <img class='paper-img' src="./static/images/efficiency_sample.png"/> -->
            
            
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section" id="Cite">
  <div class="container is-max-desktop content">
    <!--
      <h2>Try</h2>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <img class="paper-img" src="./static/images/jax_code.png"/>
      </div>
    </div>
      <h2>Read</h2>
      <a href="https://openreview.net/pdf?id=PczQtTsTIX">
      <ol class="paper-strip">
        <li><img src="./static/images/paperimg/paperimg-01.png" /> </li>
        <li><img src="./static/images/paperimg/paperimg-02.png" /> </li>
        <li><img src="./static/images/paperimg/paperimg-03.png" /> </li>
        <li><img src="./static/images/paperimg/paperimg-04.png" /> </li>
        <li><img src="./static/images/paperimg/paperimg-05.png" /> </li>
        <li><img src="./static/images/paperimg/paperimg-06.png" /> </li>
        <li><img src="./static/images/paperimg/paperimg-07.png" /> </li>
        <li><img src="./static/images/paperimg/paperimg-08.png" /> </li>
        <li><img src="./static/images/paperimg/paperimg-09.png" /> </li>
      </ol>
    </a>
    -->
    <h2>BibTeX</h2>
    <pre><code>
@inproceedings{
  <b>htang2024improving</b>,
  title={Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn},
  author={Hongyao Tang and Glen Berseth},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/pdf?id=cQoAgPBARc}
}
    </code></pre>
  </div>
</section>




<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align: center;">
            design adapted from <a
              href="https://aditya.bhatts.org/CrossQ/">CrossQ</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>